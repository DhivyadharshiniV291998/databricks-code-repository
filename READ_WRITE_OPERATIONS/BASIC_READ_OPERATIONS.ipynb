{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a33a44e8-9a58-48cc-8f6c-a20a4ab7da31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#THIS NOTEBOOK- CONTAINS BASIC READ OPERATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48780ee4-2309-447b-97ae-4ecabf7c6784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Create - Catalog(Workspace)/schema(database)/volume(Datalake (DBFS))/folder/mere data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c74511f-a85e-41b0-aa98-6223dc0b6643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists we47catalog;\n",
    "create schema if not exists we47catalog.we47schema;\n",
    "create volume if not exists we47catalog.we47schema.we47volume;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc00c6e6-3c72-4cfe-bfa5-06c9edd3833d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Create Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "700f7b48-82b4-45fe-8f91-ec5c95284d4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5ec6934-b6ad-4e66-bdd3-21f42b35a6ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Create some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7128edb1-916e-40b5-821d-ad1b82065776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_data=\"welcome to databricks pyspark learning\"\n",
    "dbutils.fs.put(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/sample.txt\",sample_data,True)\n",
    "dbutils.fs.ls(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae175c7c-0c8c-471b-a177-12eeff7cb3af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.head(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/sample.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ac1e1b3-32d7-4433-a4b8-ecafde051386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d59e42e0-a4e0-479f-915b-a3d55a7fcf52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**We are going to now learn - How to Read/Extract the data from the filesytem and load it into the distributed memory for further processing/load - using diffent methodologies/options from different sources(fs & db) and different builtin function formats (csv/json/orc/parquet/delta/tables)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06cc2063-2be5-4b11-a5f5-7cf86343f00b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.csv(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/sample.txt\")\n",
    "display(df1)\n",
    "df1.show(20,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d98601-449f-41b8-9a6b-03655f835484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc296b61-e649-45ab-930a-a30d0b30b870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##lets learn about CSV function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "091b65d7-6ab0-4049-8b7c-bb7fb994930d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "the below code by default considering c0,c1,c2.....cn as column name of the table as there is no column name given in this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b0a4319-77f1-4de5-805d-0a2f1e593160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.csv(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/custs\")\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ce755e9-1409-43c3-8961-a96b5feff7af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "the below code helps to add column name manually using .toDF method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42aefb6-8823-4d6d-b70b-4e9e50bd24f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.csv(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/custs\").toDF(\"custID\",\"fname\",\"lname\",\"age\",\"profession\")\n",
    "display(df1.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "865771b3-fb31-4a6a-99f9-73ffb4cea878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "the below code used another table(custs_header) which has its own column name, but the code takes the column name also as one of the data row of the table. to solev this, we have use header=True in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8206f4-7e76-4229-b5d5-7c178c47fdaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.csv(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/custs_header\")\n",
    "display(df1.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78df921d-ef95-490c-a748-69c317c9de0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.csv(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/custs_header\",header=True)\n",
    "display(df1.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "323ed98c-0b60-41dd-aee2-79104843d16b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We use header option to consider the column name from the data itself and we can override the source given columns to our custom column names also by using header+ toDF()..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38bd6a2a-919b-4f17-8307-c4f907a537f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.csv(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/custs_header\",header=True).toDF(\"ID\",\"F_name\",\"L_name\",\"Age\",\"Occupation\")\n",
    "display(df1.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2a3282a-c8b7-46ff-a4a5-6d4a5a8eddf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16ff6ae5-32cc-4f6e-bcb0-37801d33f8c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "but if we see the datatype of all the column is string by default, but there are 2 integer type column in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7f6ee52-6f41-4bfd-9126-93f50ab27a9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.csv(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/custs_header\",header=True,inferSchema=True).toDF(\"ID\",\"F_name\",\"L_name\",\"Age\",\"Occupation\")\n",
    "display(df1.limit(20))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "707f937e-4b4d-4959-9e60-851d81f3d647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "the below code used a table called custs_header_oth_del which has some other delimiter in the data (~) instead of the default one (,). hence, the code is considering all the data in single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abef4e2e-b06b-4e1c-b908-6cd125bfa5ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.csv(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/custs_header_oth_del\",header=True)\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0b22450-ee94-49a6-b929-4993132eb9a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "solving the above issue using the delimiter function in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d0d7091-feb8-4180-b4d9-5b9ba98b0626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.csv(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/custs_header_oth_del\",header=True,sep='~',inferSchema=True)\n",
    "display(df1)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a356a92c-f791-4186-a51f-ef4e1e4d683d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##learning option, options, format and load method to read the file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2595fac-b2bc-4ec0-92bf-b8170129854a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Using different options to create dataframe with csv and other module... (2 methodologies (spark.read.inbuiltfunction or spark.read.format(anyfunction).load(\"path\")) with 3 ways of creating dataframes (pass parameters to the csv()/option/options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0392341-1ad6-47cf-ae79-fac3c5cb601c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#general method to read the data from a file\n",
    "df1=spark.read.csv(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/custs_header_oth_del\",header=True,sep='~',inferSchema=True)\n",
    "print(df1.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c43fd019-d59a-4151-beee-7c0b066920b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#format and load method to read the data from a file\n",
    "df1_bq=spark.read.format(\"csv\").load(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/custs_header_oth_del\",header=True,sep='~',inferSchema=True)\n",
    "display(df1_bq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33581dec-44e8-4e05-bbc1-f7144b8ac7b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#using format, load and option to read the data from a file\n",
    "df1_bq=spark.read.format(\"csv\").option(\"header\",\"True\").option(\"sep\",\"~\").load(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/custs_header_oth_del\")\n",
    "display(df1_bq)\n",
    "df1_bq.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c00484-52fc-454d-9b9d-17ccb72cff0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#using format, load and options to read the data from a file\n",
    "df1_bq=spark.read.options(header=\"True\",sep=\"~\",inferschema=True).format(\"csv\").load(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/custs_header_oth_del\")\n",
    "display(df1_bq)\n",
    "df1_bq.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de00365d-9806-4f7f-a9ec-f869fbca6114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#another format without load and format function\n",
    "df1_bq=spark.read.options(header=\"True\",sep=\"~\",inferschema=True).csv(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/custs_header_oth_del\")\n",
    "display(df1_bq)\n",
    "df1_bq.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c011b6c-b779-4167-87b3-791417049ed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Reading data from multiple files & Multiple Path (We still have few more options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9a390cd-1deb-4758-9aab-3f0c86a7f495",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_multiple_files=spark.read.csv(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/custs*\",inferSchema=True,header=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "print(df1_multiple_files.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f070975-d0b0-420e-b5da-ecf5c23d20f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "What if we have files in multiple names in multiple paths... Below code can read data from multiple paths and multiple files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b40fecdc-9fcb-4d34-b469-f061856137a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_multiple_files=spark.read.csv([\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/custs*\",\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/prospect\"],inferSchema=True,header=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "print(df1_multiple_files.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27c2125f-8864-40a6-b019-6de0fe22504f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "getting data from different source systems of different regions (NY, TX, CA) into different landing pad (locations), how to access this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6e5a3a8-5039-4152-ba88-fb1a7d21775f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#recursiveFileLookup is a parameter that can be used to read data from multiple subdirectories\n",
    "#pathGlobFilter is the parameter used for matching a given pattern under the main/subdirectories\n",
    "df1_diff_subdirs=spark.read.csv(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1\",inferSchema=True,recursiveFileLookup=True,pathGlobFilter=\"custs*\").toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "print(df1_diff_subdirs.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "204ad4b1-93f0-423c-a6aa-895d280d52ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##To provide schema (columnname & datatype), what are the 2 basic options available that we learned so far ? inferSchema/toDF<br>\n",
    "We are going to learn additionally 2 more options to handle schema (colname & datatype)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7815661d-15c0-4673-86bb-7230abf54a7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1. Using simple string format of define schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f599b080-2ad5-4acc-9123-3afb25364dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datastruct=\"id int,fname string,lname string, age int,prof string\"\n",
    "df1=spark.read.schema(datastruct).csv(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/custs\")\n",
    "display(df1)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fb0443b-97ac-4945-a522-c4554004ef32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2. Using structure type to define schema(IMPORTANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d508f1e6-1dae-4623-9af7-270183c20c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "custom_schema=StructType([StructField(\"id\",IntegerType(),False),StructField(\"fname\",StringType()),StructField(\"lname\",StringType()),StructField(\"age\",IntegerType()),StructField(\"profession\",StringType())])\n",
    "df1=spark.read.schema(custom_schema).csv(\"/Volumes/we47catalog/we47schema/we47volume/we47dir1/custs\")\n",
    "display(df1.limit(10))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0662c3e-49a7-4c07-8166-7e49d1a8388c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Reading data from different builtin formats (csv, json, xml, orc, parquet, delta, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8023ed3c-5d52-49ed-9bee-0f2df1792598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1.Reading data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d2ce7a3-cb8e-494b-a00d-db6165bd3ed6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.csv(\"/Volumes/we47catalog/we47schema/we47volume/serialized_compressed_data_sources/csv_targetdata/\",header=True,sep=\"~\",inferSchema=True)\n",
    "display(df1.limit(10))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c48efaee-e95e-4593-8586-dcc3dff812b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2.Reading data from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "359f7857-76d8-45d7-9bca-ae292192cb2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.format(\"json\").load(\"/Volumes/we47catalog/we47schema/we47volume/serialized_compressed_data_sources/json_targetdata/\")\n",
    "display(df1.limit(10))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0697eab-0a0c-413b-b58a-f4c1617fb3ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###3.Reading data from xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87efb156-8f5d-4db5-b8a3-d9d5255424a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.xml(\"/Volumes/we47catalog/we47schema/we47volume/serialized_compressed_data_sources/xml_targetdata/\",rowTag=\"customer\")\n",
    "display(df1.limit(10))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f9d9411-800d-4dd3-94a7-aca282a9b147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###4.Reading data from Serialized format - orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df10a7a-294b-437a-a2bc-9d53ec763161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.orc(\"/Volumes/we47catalog/we47schema/we47volume/serialized_compressed_data_sources/orc_targetdata/\")\n",
    "display(df1.limit(10))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5842ad0-6e04-42ed-a555-58dcccb45e8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###5.Reading data from Serialized format - parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d4d2d18-3194-4aeb-8099-d31f74c4e1a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.parquet(\"/Volumes/we47catalog/we47schema/we47volume/serialized_compressed_data_sources/parquet_targetdata/\")\n",
    "display(df1.limit(10))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d88def5e-c453-43cf-93f4-3f5ce8a0b3b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###6.Reading data from Serialized format - delta(parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "453ce876-77e1-45c8-9eb3-a00b6ec3c26f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.format(\"delta\").load(\"/Volumes/we47catalog/we47schema/we47volume/serialized_compressed_data_sources/delta_targetdata/\")\n",
    "display(df1.limit(10))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62653136-1123-492a-8dff-e6df71c85267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###7.Reading data from table - delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9384a161-e5d7-4ce9-9d49-e15c83bbf674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.table(\"default.customertbl\")\n",
    "display(df1.limit(10))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44bc4cac-f3b0-404b-a0ef-ac836180cee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2=df1.select(\"age\",\"custid\",\"fname\",\"lname\",\"profession\").filter(\"age=55\")\n",
    "display(df2.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca6fe18-e09d-46c2-911f-c01a8b121a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2=df1.select(\"age\",\"custid\",\"fname\",\"lname\",\"profession\").filter(\"profession= 'Pilot' \")\n",
    "display(df2)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6602422024950195,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BASIC_READ_OPERATIONS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
