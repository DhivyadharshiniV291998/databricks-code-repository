{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05a165a1-1654-4dc5-a31e-adebb2333fdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#WHAT IS DATA MUNGING?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9769f6c-344c-4be7-a700-ca003b0e6f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Process of transforming and mapping data from Raw form into Tidy(usable) format with the intent of making it more appropriate and valuable for a variety of downstream purposes such for further Transformation/Enrichment, Egress/Outbound, analytics, Datascience/AI application & Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3364763f-dedf-4f8b-8807-9bc2ee82ccdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#TYPES OF DATA MUNGING:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2765a6f6-e791-447f-b4b1-535c6f9e8214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1.Passive munging - visual analyzation of data using mere eyes. by looking at the data, what we analyze manually.<br>\n",
    "2.Active munging - basic programmatic way of analyzation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1afb9a8d-e158-4758-97a0-6b422cfadeb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1.PASSIVE DATA MUNGING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e25f149-9ba9-4c93-b7da-f3610545e263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- It is a Structured data with comma seperator (CSV)\n",
    "- No Header, No comments, footer is there in the data\n",
    "- Total columns are (seperator + 1)\n",
    "- Data Quality\n",
    "- >Null columns & null rows are there\n",
    "- >duplicate rows & Duplicate keys\n",
    "- >format issues are there (age is not in number format eg. 7-7)\n",
    "- >Uniformity issues (Artist, artist)\n",
    "- >Number of columns are more or less than the expected\n",
    "- >eg. 4000011,Francis,McNamara,47,Therapist,NewYork & 4000014,Beth,Woodard,65\n",
    "- >Identification of data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1551415e-04b6-4b36-8434-12f06b3d1252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark=SparkSession.builder.appName(\"we47_Bread_n_Butter2_Important_Application\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fece13b0-05f0-442d-a288-34bbd15d004b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df1=spark.read.csv(\"/Volumes/workspace/default/volumewe47_datalake/custsmodified\",inferSchema=True,header=False).toDF(\"id\",\"fname\",\"lname\",\"age\",\"profession\")\n",
    "raw_df1.show(20,False)\n",
    "display(raw_df1.limit(20))\n",
    "raw_df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b06f2d0a-4731-4a70-b49e-a5310ae6a944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##very important\n",
    "raw_df1.printSchema()\n",
    "print(raw_df1.columns)\n",
    "print(raw_df1.dtypes)\n",
    "print(raw_df1.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3352c749-5f1d-4079-9646-4a7c06cb1661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"actual count of the data\",raw_df1.count())\n",
    "print(\"de-duplication count of the data\",raw_df1.dropDuplicates().count())#Can remove duplicates based on: all coulmns or selected columns\n",
    "print(\"distinct count of the data\",raw_df1.distinct().count())#Removes exact duplicate rows\n",
    "print(\"de-duplicated given cid column count\",raw_df1.dropDuplicates(['id']).count())#removing duplicates based on cid column\n",
    "display(raw_df1.describe())\n",
    "display(raw_df1.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "101b7253-4ac9-4c4a-88b6-bedb0c5fcd92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2.ACTIVE DATA MUNGING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f055ad70-be85-4ea1-a5d0-d9588c783ac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Combining Data + Schema Evolution/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4a6aac9-38df-4ced-a33e-d2dbd82d7cdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1.Combining Data + Schema Evolution/Merging (Structuring) - Preliminary Datamunging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81e2976-bccb-44f5-9e1e-d310505b9a0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "struct1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "#1.single file\n",
    "raw_df1=spark.read.csv(\"/Volumes/workspace/default/volumewe47_datalake/custsmodified\",schema=struct1)\n",
    "display(raw_df1.limit(20))\n",
    "#2.multiple files with diff names\n",
    "raw_df1=spark.read.csv([\"/Volumes/workspace/default/volumewe47_datalake/custsmodified\",\"/Volumes/workspace/default/volumewe47_datalake/custsmodifiedsample.txt\"],schema=struct1)\n",
    "display(raw_df1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1800d689-672e-466d-9975-0235b5e97941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####A. COMBINING OR SCHEMA MERGING or SCHEMA MELTING of Data from different sources(union,unionByName,allowMissingColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "749f5903-3254-421c-9b50-e9dda16f83ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "best if we keep on getting variable data from the source regularly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8ddee99-7181-4afd-8c1a-46ab05dd386f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "strt1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(strt1).csv(path=[\"/Volumes/workspace/default/volumewe47_datalake\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_N*\")\n",
    "strt2=\"id string, firstname string, age string, profession string,city string\"\n",
    "rawdf2=spark.read.schema(strt2).csv(path=[\"/Volumes/workspace/default/volumewe47_datalake\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_T*\")\n",
    "display(rawdf1)\n",
    "display(rawdf2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf315e76-2221-4168-b85d-1e5f94262c01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####note:\n",
    "**UNION** function can also be used for merging/melting of the dataset when we know that the number and name of both the datasets are same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b457da21-8d68-4908-89fd-ea073a7e82a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf_merged=rawdf1.unionByName(rawdf2,allowMissingColumns=True)\n",
    "display(rawdf_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00d03081-c43b-4803-b2ee-ec562e5d6bf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####B. schema evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b17ba3e-efb4-4496-8b63-88e061a2eb15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "it is best to use schema evolution method if we get variable data from the source over a very random period of time (not regularly, very often)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dfe0ac8-852f-4344-9352-042eacbdb408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "strt1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(strt1).csv(path=[\"/Volumes/workspace/default/volumewe47_datalake\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_N*\")\n",
    "strt2=\"id string, firstname string, age string, profession string,city string\"\n",
    "rawdf2=spark.read.schema(strt2).csv(path=[\"/Volumes/workspace/default/volumewe47_datalake\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_T*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba6abb1c-ea72-4286-84ee-4c2b26149c0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1.write.orc(\"/Volumes/workspace/default/volumewe47_datalake/targetorc1/\",mode='overwrite')#Useful for future over the time\n",
    "rawdf2.write.orc(\"/Volumes/workspace/default/volumewe47_datalake/targetorc1/\",mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c73eb45c-2aee-4e35-979c-8603af932efd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf_evolved=spark.read.orc(\"/Volumes/workspace/default/volumewe47_datalake/targetorc1/\",mergeSchema=True)\n",
    "display(rawdf_evolved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddfa0c1c-368c-4925-97d6-502516f5daff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6b29d00-45d4-492b-bb4f-e75f6922574f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdda2774-f4fc-48a5-880c-80b470153ad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####method-1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd7a76ae-aa62-46b0-89ad-737493c23278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#strt1=\"id int, firstname string, lastname string, age int, profession string\"\n",
    "#method-1:permissive with all rows with respective nulls\n",
    "strt11=StructType([StructField(\"id\",IntegerType(),True),StructField(\"firstname\",StringType(),True),StructField(\"lastname\",StringType(),True),StructField(\"age\",IntegerType(),True),StructField(\"profession\",StringType(),True)])\n",
    "dfmethod1=spark.read.schema(strt11).csv(\"/Volumes/workspace/default/volumewe47_datalake/custsmodified\",mode=\"PERMISSIVE\",header=False)\n",
    "#We are making nulls where ever data format mismatch is there (cutting down mud portition from potato - scrubbing)\n",
    "display(dfmethod1)\n",
    "dfmethod1.printSchema()\n",
    "print(\"entire count of data\",dfmethod1.count())\n",
    "print(\"after scrubbing, count of data\",len(dfmethod1.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2dcaca1-a4d2-4335-b236-8249f6c4b0e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####method-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea4cce59-66ec-4ce3-9f9f-5246bc1aded4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#method2 - drop malformed rows\n",
    "dfmethod2=spark.read.schema(strt11).csv(\"/Volumes/workspace/default/volumewe47_datalake/custsmodified\",mode=\"dropMalformed\",header=False)\n",
    "#We are removing the entire row, where ever data format mismatch is there (number of columns, data type mismatch) (throwing away the entire potato or some portion of it by - cleansing)\n",
    "display(dfmethod2)\n",
    "print(\"entire count of data\",dfmethod2.count())\n",
    "print(\"after cleansing, count of data\",len(dfmethod2.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc01d86-e438-42be-b465-4592ae685f68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfmethod2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0963f9aa-eaf9-4d1c-a672-184f44a47a85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####method-3: BEST OPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d8461b-95d9-4f5b-9a49-72c3be38aab5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If we follow method1 (permissive with strict schema verification) or method2 (drop malformed with strict schema verification)<BR>\n",
    "Challenges we have in this method1 and 2 is - unknown data loss at column level or at the row level<BR>\n",
    "method3 best methodology of applying active data munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8dbe9ef-eb48-4542-a01f-69be6e849f84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfmethod3=spark.read.csv(\"/Volumes/workspace/default/volumewe47_datalake/custsmodified\",mode=\"PERMISSIVE\",header=False,inferSchema=True)\n",
    "print(dfmethod3.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df6c2249-f25b-4186-946b-a15a66a5c8d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "strt11=StructType([StructField(\"id\",StringType(),True),StructField(\"firstname\",StringType(),True),StructField(\"lastname\",StringType(),True),StructField(\"age\",StringType(),True),StructField(\"profession\",StringType(),True)])\n",
    "dfmethod3=spark.read.schema(strt11).csv(\"/Volumes/workspace/default/volumewe47_datalake/custsmodified\",mode=\"PERMISSIVE\",header=False)\n",
    "display(dfmethod3)\n",
    "dfmethod3.printSchema()\n",
    "print(\"entire count of data\",dfmethod3.count())\n",
    "print(\"after cleansing, count of data\",len(dfmethod3.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff346693-e9d0-4e39-b86b-5f31e323298c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Rejection strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10aaeb21-683f-436a-96b4-fa8a456abfde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "strt11=StructType([StructField(\"id\",IntegerType(),True),StructField(\"firstname\",StringType(),True),StructField(\"lastname\",StringType(),True),StructField(\"age\",IntegerType(),True),StructField(\"profession\",StringType(),True),StructField(\"corruptdata\",StringType())])\n",
    "dfmethod3=spark.read.schema(strt11).csv(\"/Volumes/workspace/default/volumewe47_datalake/custsmodified\",mode=\"PERMISSIVE\",header=False,columnNameOfCorruptRecord=\"corruptdata\")\n",
    "display(dfmethod3)\n",
    "print(\"entire count of data\",dfmethod3.count())\n",
    "df_reject=dfmethod3.where(\"corruptdata is not null\")\n",
    "display(df_reject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdace4d1-21fc-4f7a-a883-aa1f2ba94e5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_reject.drop(\"corruptdata\").write.mode(\"overwrite\").csv(\"/Volumes/workspace/default/volumewe47_datalake/rejects/\",mode=\"overwrite\")\n",
    "print(\"Data to reject or update the source\",len(dfmethod3.where(\"corruptdata is not null\").collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c94598f-3950-4585-a9ed-d39b6f1a0cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####cleansing stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59cf68d6-938a-4dab-9668-4f6cc1274c0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Can be achieved using 2 functions under na (na.drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd51d368-f69a-4459-b7ab-cc20fe8d4677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#actual DF count befor doing any cleansing activity\n",
    "strt11=StructType([StructField(\"id\",StringType(),True),StructField(\"firstname\",StringType(),True),StructField(\"lastname\",StringType(),True),StructField(\"age\",StringType(),True),StructField(\"profession\",StringType(),True)])\n",
    "dfmethod3=spark.read.schema(strt11).csv(\"/Volumes/workspace/default/volumewe47_datalake/custsmodified\",mode=\"PERMISSIVE\",header=False)\n",
    "display(dfmethod3.take(15))\n",
    "print(\"Actual DF count\",len(dfmethod3.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "931db05c-9b8c-4fff-98c8-f0733c6e95da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#applying cleansing strategies\n",
    "cleansed_df1=dfmethod3.na.drop(how=\"any\")#drop the row, if any one column in our df row contains null\n",
    "cleansed_df1=dfmethod3.na.drop(how=\"any\",subset=[\"id\",\"age\"])#drop the row, if any one column id/age contains null\n",
    "print(\"cleansed any DF count\",len(cleansed_df1.collect()))\n",
    "display(cleansed_df1.take(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "357eef57-6bb2-4632-8b31-252ca8aec8ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleansed_df2=dfmethod3.na.drop(how=\"all\")#drop the row, if all the columns in our df row contains null\n",
    "cleansed_df2=dfmethod3.na.drop(how=\"all\",subset=[\"id\",\"profession\"])#drop the row, if all the columns (id,profession) in our df row contains null\n",
    "print(\"cleansed all DF count\",len(cleansed_df2.collect()))\n",
    "display(cleansed_df2.take(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2f383ab-d80b-4a27-9ba4-3babf868ae3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Threshold - least bother (if we need minimum this many number of columns with not nulls, then only we will keep the row)\n",
    "cleansed_df3=dfmethod3.na.drop(thresh=4,subset=[\"id\",\"firstname\",\"age\",\"profession\"])#only allowing null at lastname column\n",
    "print(\"cleansed threshold DF count\",len(cleansed_df3.collect()))\n",
    "display(cleansed_df3.take(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d6a83c9-8f31-43ca-8575-9c57cf382da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Before scrubbing, lets take the right cleansed data with id as null and entire row as null removed out\n",
    "#Finally I am arriving for our current data, lets perform the best cleansing\n",
    "cleansed_df=cleansed_df3.na.drop(subset=[\"id\"]).na.drop(how=\"all\")\n",
    "cleansed_df=cleansed_df.na.drop(subset=[\"firstname\",\"lastname\"],how='all')\n",
    "print(\"Final cleansed DF\",len(cleansed_df.collect()))\n",
    "display(cleansed_df.take(15))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1318884f-b227-4bef-a10f-c67e83b67089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####scrubbing stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e62b1b35-3554-4110-a48b-b77b6ade9bf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Can be achieved using 2 functions under na (na.fill & na.replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bb18eb8-0fd5-408e-b76e-0dc4d02a3ea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scrubbed_df1=cleansed_df.na.fill(\"na\",subset=[\"firstname\",\"lastname\"]).na.fill(\"not provided\",subset=[\"profession\"])\n",
    "scrubbed_df2=scrubbed_df1.na.replace(\"IT\",\"Information Technologies\",subset=[\"profession\"]).na.replace(\"Pilot\",\"Aircraft Pilot\",subset=[\"profession\"])\n",
    "display(scrubbed_df2.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdf7d9e3-d046-4732-a73d-baf84eb963d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#or\n",
    "#Find and Replace functionality\n",
    "dict1={\"IT\":\"Information Technologies\",\"Pilot\":\"Aircraft Pilot\",\"Actor\":\"Celebrity\"}\n",
    "scrubbed_df=scrubbed_df1.na.replace(dict1,subset=[\"profession\"])\n",
    "#scrubbed_df2=scrubbed_df1.na.replace(\"IT\",\"Information Technologies\",subset=[\"profession\"]).na.replace(\"Pilot\",\"Aircraft Pilot\",subset=[\"profession\"])\n",
    "print(\"scrubbed DF\",len(scrubbed_df.collect()))\n",
    "display(scrubbed_df.take(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61d24aac-ea64-4c47-be03-3f6e57661d6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "128deba8-5b2f-477a-b22e-2ec0ce45bf5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Standardization1 - Column Enrichment (Addition of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2464e62-937b-4f24-81af-5d13699dfe6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,initcap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e140ed8d-d1d4-49fd-b4b3-ca48e49ee38c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "lit() is used to create a column with a constant or literal value so it can be added to a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8346f0d-d3ac-4034-997a-844081f4adcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standard_df1=scrubbed_df.withColumn(\"sourcesystem\",lit(\"Retail\")) \n",
    "display(standard_df1.take(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "094c1a59-98a0-40d6-a69b-14efc9c5116d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Standardization2 - Column Uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9623485e-6c15-44e2-b068-7e73549d5325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#DSL\n",
    "display(standard_df1.groupBy(\"profession\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "582984e1-0f58-44bf-8f37-d80c8dc6bdfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Declarative lang\n",
    "standard_df1.createOrReplaceTempView(\"view1\")\n",
    "display(spark.sql(\"select profession,count(1) from view1 group by profession\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "541b37fa-ab3a-4283-9a54-32bc46f23c85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standard_df2=standard_df1.withColumn(\"profession\",initcap(\"profession\"))\n",
    "display(standard_df2.take(15))\n",
    "display(standard_df2.groupBy(\"profession\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fb48655-f428-4772-b9de-551561e83c2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Standardization3 - Format Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b9767e-48d5-4ccb-8407-779a2e1d8bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "cid_standardization={\"one\":\"1\",\"two\":\"2\",\"ten\":\"10\"}#We can think of using GenAI here later\n",
    "standard_df3=standard_df2.na.replace(cid_standardization,subset=[\"id\"])#Using munging feature for standardizing the data\n",
    "standard_df3=standard_df3.withColumn(\"age\",regexp_replace(\"age\",\"-\",\"\"))\n",
    "display(standard_df3.take(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a876b483-8e14-4060-8621-4a1fc66a126b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Standardization4 - Type Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23d1584d-96aa-4ed7-a132-0be9b8f1b250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**TEST-CODE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cfde25e-12af-4834-923b-f7f6c7bff850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#I wanted to learn/test a function functionality\n",
    "#create a dummy dataframe, apply function to that dummy df to test your function's functionality\n",
    "spark.sql(\"select '12a3' as col1\").where(\"col1 not rlike '[a-zA-Z]'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "382b2d43-ff79-4e4e-be66-943508995f4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standard_df3.printSchema()#still the datatype of 'id' column is string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "878c3c76-f449-469e-893e-bd2dcf58a2a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standard_df3=standard_df3.where(\"id not rlike '[a-zA-Z]'\")#Removed the string data in the id column\n",
    "display(standard_df3.where(\"id='trailer_data:end of file'\"))\n",
    "standard_df4=standard_df3.withColumn(\"age\",col(\"age\").cast(\"int\")).withColumn(\"id\",col(\"id\").cast(\"long\"))\n",
    "standard_df4.printSchema()\n",
    "display(standard_df4.take(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c6319c6-f9b1-46a4-bbe6-c51a3116d0e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Standardization5 - Naming Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74f66408-2d5c-40f1-bc89-17d0ccd4f616",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standard_df5=standard_df4.withColumnRenamed(\"id\",\"custid\")\n",
    "standard_df5.printSchema()\n",
    "display(standard_df5.take(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b78f6d9-5dbc-448e-86e5-9e6239e02f0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Standardization6 - Reorder Standadization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b455f33-257a-4b9d-b270-d3f8de6aa153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standard_df6=standard_df5.select(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"sourcesystem\")\n",
    "standard_df6.printSchema()\n",
    "display(standard_df6.take(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efbdc78d-155a-4a83-bff8-7945a057f030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#DeDuplication - De-Duplication and removal of non prioritized dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cacb607b-a6b4-4bac-827d-4bf4aefeb119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Duplicate Elimination at the record level, column level and in a priority basis of some column level higher age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f68226c4-3c95-4947-a4fe-e38a77c24659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dedup_df1=standard_df6.where(\"custid in (4000001,4000003)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28e53ed1-cd65-4475-a086-c7b3d439a25e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dedup_df1=standard_df6.distinct()#Eliminating Record level duplicates\n",
    "dedup_df2=dedup_df1.dropDuplicates(subset=[\"custid\"])#Retains only the first row and eliminate the subsequent rows with duplicate keys without having any other priority at the age or any other columns\n",
    "display(dedup_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5913a3d8-e1a8-46d1-9fa3-9625a9011fbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dedup_df4=dedup_df1.dropDuplicates([\"custid\",\"age\",\"firstname\",\"lastname\"]).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.DATA_MUNGING",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
